{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentimen Analisis Model Indo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Doc2Vec\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "import ipython_genutils\n",
    "import pickle\n",
    "import progressbar\n",
    "import tensorflow as tf\n",
    "from gensim.models import KeyedVectors\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import model_from_json\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, MaxPooling1D,GRU,LSTM\n",
    "from keras.layers import Flatten\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import model_selection\n",
    "from sklearn import utils\n",
    "from sklearn.utils import shuffle\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load file csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "target\n",
    "1. negative = -1\n",
    "2. neutral = 0\n",
    "3. positive = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('lib/dataset',index_col=0)\n",
    "df1 = pd.read_csv('lib/dataset ahok',index_col=0)\n",
    "df2 = pd.read_csv('lib/datasetfm',index_col=0)\n",
    "df=pd.concat([df,df1,df2],sort=False)\n",
    "#df['target']=df['target'].map({0:-1,1:0,2:1})\n",
    "df.dropna(inplace=True)\n",
    "df.reset_index(drop=True,inplace=True)\n",
    "df = shuffle(df)\n",
    "df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### jumlah data tiap class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('target')['text'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### downsampling dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dneg=df.query('target == -1')\n",
    "dnet=df.query('target == 0')\n",
    "dpos=df.query('target == 1')\n",
    "\n",
    "minlen=min(len(dnet.index),len(dpos.index),len(dneg.index))\n",
    "\n",
    "\n",
    "dnet=dnet.head(minlen)\n",
    "dneg=dneg.head(minlen)\n",
    "dpos=dpos.head(minlen)\n",
    "\n",
    "data_training=pd.concat([dneg.text,dnet.text,dpos.text])\n",
    "target=pd.concat([dneg.target,dnet.target,dpos.target])\n",
    "target=to_categorical(target-target.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tanpa downsampling\n",
    "data_training=df.text\n",
    "target=df.target\n",
    "target=to_categorical(target-target.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ds=pd.DataFrame({'text':data_training,'target':np.argmax(target, axis=1, out=None)})\n",
    "df_ds.groupby('target')['text'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tokenizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create tokenizer lalu di save ada/dibuat tokenizer. jika sudah dibuat maka skip langsung ke load tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save tokenizer\n",
    "#data=df.text\n",
    "#tokenizer = Tokenizer(num_words=100000)\n",
    "#tokenizer.fit_on_texts(data)                        \n",
    "#sequences = tokenizer.texts_to_sequences(data) \n",
    "#with open('lib/tokenizer_indo.pickle', 'wb') as handle:\n",
    "#    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load tokenizer lalu konversi dataset jadi vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('tokenizing')\n",
    "num_words=100000\n",
    "tokenizer = Tokenizer(num_words)\n",
    "with open('lib/tokenizer_indo.pickle', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)\n",
    "sequences = tokenizer.texts_to_sequences(data_training) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('padding')\n",
    "input_seq = pad_sequences(sequences, maxlen=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seq[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split dataset jadi data training dan data validasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_train, input_val, target_train, target_val = model_selection.train_test_split(input_seq,target,test_size = 0.2, random_state = 0)\n",
    "\n",
    "print('data train\\t: ',len(input_train))\n",
    "print('data validasi\\t: ',len(input_val),)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. ep = epoch\n",
    "2. bs = batch size\n",
    "3. traina = True atau False \n",
    "4. lanjut = True atau False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ket.\n",
    "1. jumlah epoch training model\n",
    "2. jumlah batch size dlm training\n",
    "3. True untuk trainable sehingga dalam proses training model word embedding juga di train. False untuk word embedding tidak ikut di train.\n",
    "4. True untuk me-load model yg sudah di train sebelumnya/pretrained model lalu model tsb di train lagi. False untuk training model dari awal seperti model baru/ blm di train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ep=135\n",
    "bs=32\n",
    "traina=True\n",
    "lanjut=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model\n",
    "1. Word2Vec\n",
    "2. DBOW\n",
    "3. DMM\n",
    "4. DMC\n",
    "5. DBOW-DMM\n",
    "6. DBOW-DMC\n",
    "7. DMM-DMC\n",
    "8. DBOW-DMM-DMC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getModel(num_word,embed_dim,embed_weight,input_len,trainable):\n",
    "    seed = 7\n",
    "    model = Sequential()\n",
    "    e = Embedding(num_word, embed_dim, weights=[embed_weight], input_length=input_len, trainable=trainable)\n",
    "    model.add(e)\n",
    "    model.add(Conv1D(32,kernel_size=2,padding='same',activation='relu'))\n",
    "    model.add(MaxPooling1D())\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Conv1D(64,kernel_size=2,padding='same',activation='relu'))\n",
    "    model.add(MaxPooling1D())\n",
    "    model.add(Dropout(0.35))\n",
    "    model.add(Conv1D(128,kernel_size=2,padding='same',activation='relu'))\n",
    "    model.add(MaxPooling1D())\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(LSTM(50,return_sequences=True))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256,activation='relu'))\n",
    "    model.add(Dropout(0.45))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveModel(model,path):\n",
    "    model_json = model.to_json()\n",
    "    with open(path, \"w\") as json_file:\n",
    "        json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,lanjut, path_weights,input_train,target_train,input_val,target_val,epoch,bs):\n",
    "    if lanjut==True:\n",
    "        model.load_weights(path_weights)\n",
    "    filepath=path_weights\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_categorical_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "    callbacks_list = [checkpoint]\n",
    "    history=model.fit(input_train, target_train,validation_data=(input_val, target_val), shuffle=False, epochs=ep, batch_size=bs,callbacks=callbacks_list)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(weights,input_val,target_val):\n",
    "    model.load_weights(weights)\n",
    "    scores = model.evaluate(input_val, target_val)\n",
    "    print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "    print()\n",
    "    predict = model.predict_classes(input_val)\n",
    "    y_true = np.argmax(target_val, axis=1, out=None)\n",
    "    y_pred = predict\n",
    "    target_names = ['negative', 'neutral', 'positive']\n",
    "    print (classification_report(y_true, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(history):\n",
    "    # Plot training & validation accuracy values\n",
    "    plt.plot(history.history['categorical_accuracy'])\n",
    "    plt.plot(history.history['val_categorical_accuracy'])\n",
    "    plt.title('Model accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot training & validation loss values\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load word2vec\n",
    "model_ug_cbow = KeyedVectors.load('lib/cbow_indo.word2vec')\n",
    "model_ug_sg = KeyedVectors.load('lib/sg_indo.word2vec')\n",
    "print('word2vec loaded')\n",
    "\n",
    "# words embedding\n",
    "embeddings_index = {}\n",
    "for w in model_ug_cbow.wv.vocab.keys():\n",
    "    embeddings_index[w] =np.append( model_ug_cbow.wv[w],model_ug_sg.wv[w])\n",
    "\n",
    "num_words = 100000\n",
    "embedding_matrix = np.zeros((num_words, 200))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i >= num_words:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "print('embed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=getModel(num_words, 200, embedding_matrix, 70, traina)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveModel(model,\"lib/model_w2v_indo.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history=train(model,lanjut, 'lib/weights_w2v.hdf5',input_train,target_train,input_val,target_val,ep,bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score('lib/weights_w2v.hdf5',input_val,target_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ug_cbow = Doc2Vec.load('lib/d2v_dbow_indo')\n",
    "\n",
    "print('word2vec loaded')\n",
    "\n",
    "# words embedding\n",
    "embeddings_index = {}\n",
    "for w in model_ug_cbow.wv.vocab.keys():\n",
    "    embeddings_index[w] = model_ug_cbow.wv[w]\n",
    "\n",
    "num_words = 100000\n",
    "embedding_matrix = np.zeros((num_words, 100))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i >= num_words:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "print('embed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=getModel(num_words, 100, embedding_matrix, 70, traina)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveModel(model,\"lib/model_dbow_indo.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history=train(model,lanjut, 'lib/weights_dbow.hdf5',input_train,target_train,input_val,target_val,ep,bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score('lib/weights_dbow.hdf5',input_val,target_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ug_cbow = Doc2Vec.load('lib/d2v_dmm_indo')\n",
    "\n",
    "print('word2vec loaded')\n",
    "\n",
    "# words embedding\n",
    "embeddings_index = {}\n",
    "for w in model_ug_cbow.wv.vocab.keys():\n",
    "    embeddings_index[w] = model_ug_cbow.wv[w]\n",
    "\n",
    "num_words = 100000\n",
    "embedding_matrix = np.zeros((num_words, 100))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i >= num_words:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "print('embed')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=getModel(num_words, 100, embedding_matrix, 70, traina)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveModel(model,\"lib/model_dmm_indo.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history=train(model,lanjut, 'lib/weights_dmm.hdf5',input_train,target_train,input_val,target_val,ep,bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score('lib/weights_dmm.hdf5',input_val,target_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ug_cbow = Doc2Vec.load('lib/d2v_dmc_indo')\n",
    "\n",
    "print('word2vec loaded')\n",
    "\n",
    "# words embedding\n",
    "embeddings_index = {}\n",
    "for w in model_ug_cbow.wv.vocab.keys():\n",
    "    #embeddings_index[w] =np.append( model_ug_cbow.wv[w],model_ug_sg.wv[w])\n",
    "    embeddings_index[w] = model_ug_cbow.wv[w]\n",
    "\n",
    "num_words = 100000\n",
    "embedding_matrix = np.zeros((num_words, 100))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i >= num_words:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "print('embed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=getModel(num_words, 100, embedding_matrix, 70, traina)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveModel(model,\"lib/model_dmc_indo.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history=train(model,lanjut, 'lib/weights_dmc.hdf5',input_train,target_train,input_val,target_val,ep,bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score('lib/weights_dmc.hdf5',input_val,target_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBOW - DMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ug_cbow = Doc2Vec.load('lib/d2v_dbow_indo')\n",
    "model_ug_sg = Doc2Vec.load('lib/d2v_dmm_indo')\n",
    "\n",
    "print('word2vec loaded')\n",
    "\n",
    "# words embedding\n",
    "embeddings_index = {}\n",
    "for w in model_ug_cbow.wv.vocab.keys():\n",
    "    embeddings_index[w] =np.append( model_ug_cbow.wv[w],model_ug_sg.wv[w])\n",
    "    #embeddings_index[w] = model_ug_cbow.wv[w]\n",
    "\n",
    "num_words = 100000\n",
    "embedding_matrix = np.zeros((num_words, 200))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i >= num_words:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "print('embed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=getModel(num_words, 200, embedding_matrix, 70, traina)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveModel(model,\"lib/model_dbow-dmm_indo.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history=train(model,lanjut, 'lib/weights_dbow-dmm.hdf5',input_train,target_train,input_val,target_val,ep,bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score('lib/weights_dbow-dmm.hdf5',input_val,target_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBOW - DMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ug_cbow = Doc2Vec.load('lib/d2v_dbow_indo')\n",
    "model_ug_sg = Doc2Vec.load('lib/d2v_dmc_indo')\n",
    "\n",
    "print('word2vec loaded')\n",
    "\n",
    "# words embedding\n",
    "embeddings_index = {}\n",
    "for w in model_ug_cbow.wv.vocab.keys():\n",
    "    embeddings_index[w] =np.append( model_ug_cbow.wv[w],model_ug_sg.wv[w])\n",
    "    #embeddings_index[w] = model_ug_cbow.wv[w]\n",
    "\n",
    "num_words = 100000\n",
    "embedding_matrix = np.zeros((num_words, 200))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i >= num_words:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "print('embed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=getModel(num_words, 200, embedding_matrix, 70, traina)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveModel(model,\"lib/model_dbow-dmc_indo.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history=train(model,lanjut, 'lib/weights_dbow-dmc.hdf5',input_train,target_train,input_val,target_val,ep,bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score('lib/weights_dbow-dmc.hdf5',input_val,target_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DMM - DMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ug_cbow = Doc2Vec.load('lib/d2v_dmm_indo')\n",
    "model_ug_sg = Doc2Vec.load('lib/d2v_dmc_indo')\n",
    "\n",
    "print('word2vec loaded')\n",
    "\n",
    "# words embedding\n",
    "embeddings_index = {}\n",
    "for w in model_ug_cbow.wv.vocab.keys():\n",
    "    embeddings_index[w] =np.append( model_ug_cbow.wv[w],model_ug_sg.wv[w])\n",
    "    #embeddings_index[w] = model_ug_cbow.wv[w]\n",
    "\n",
    "num_words = 100000\n",
    "embedding_matrix = np.zeros((num_words, 200))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i >= num_words:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "print('embed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=getModel(num_words, 200, embedding_matrix, 70, traina)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveModel(model,\"lib/model_dmm-dmc_indo.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history=train(model,lanjut, 'lib/weights_dmm-dmc.hdf5',input_train,target_train,input_val,target_val,ep,bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score('lib/weights_dmm-dmc.hdf5',input_val,target_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBOW - DMM - DMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ug_cbow = Doc2Vec.load('lib/d2v_dbow_indo')\n",
    "model_ug_sg = Doc2Vec.load('lib/d2v_dmm_indo')\n",
    "model_ug_sg1 = Doc2Vec.load('lib/d2v_dmc_indo')\n",
    "\n",
    "print('word2vec loaded')\n",
    "\n",
    "# words embedding\n",
    "embeddings_index = {}\n",
    "for w in model_ug_cbow.wv.vocab.keys():\n",
    "    embeddings_index[w] =np.append((np.append(model_ug_cbow.wv[w],model_ug_sg.wv[w])),model_ug_sg1.wv[w])\n",
    "    \n",
    "\n",
    "num_words = 100000\n",
    "embedding_matrix = np.zeros((num_words, 300))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i >= num_words:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "print('embed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=getModel(num_words, 300, embedding_matrix, 70, traina)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveModel(model,\"lib/model_dbow-dmm-dmc_indo.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history=train(model,lanjut, 'lib/weights_dbow-dmm-dmc.hdf5',input_train,target_train,input_val,target_val,ep,bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score('lib/weights_dbow-dmm-dmc.hdf5',input_val,target_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
